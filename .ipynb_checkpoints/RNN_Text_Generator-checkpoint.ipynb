{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGPzaxTp75Ls"
   },
   "source": [
    "<a \n",
    "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
    "  target=\"_parent\">\n",
    "  <img\n",
    "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
    "    alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cksgAH12XRjV"
   },
   "source": [
    "# Sequence-to-sequence models\n",
    "\n",
    "### Description:\n",
    "I developed this lab using code from the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
    "\n",
    "\n",
    "### Example Output:\n",
    "An example of my final samples are shown below (more detail in the\n",
    "final section of this writeup), after 150 passes through the data.\n",
    "\n",
    "<code>\n",
    "And ifte thin forgision forward thene over up to a fear not your\n",
    "And freitions, which is great God. Behold these are the loss sub\n",
    "And ache with the Lord hath bloes, which was done to the holy Gr\n",
    "And appeicis arm vinimonahites strong in name, to doth piseling \n",
    "And miniquithers these words, he commanded order not; neither sa\n",
    "And min for many would happine even to the earth, to said unto m\n",
    "And mie first be traditions? Behold, you, because it was a sound\n",
    "And from tike ended the Lamanites had administered, and I say bi\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7bdZWxvJrsx",
    "outputId": "2ab0060d-d673-4750-95bc-e0183ddf06b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-19 04:23:17--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
      "Resolving piazza.com (piazza.com)... 18.215.222.38, 52.205.194.150, 52.206.193.161, ...\n",
      "Connecting to piazza.com (piazza.com)|18.215.222.38|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
      "--2023-02-19 04:23:18--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
      "Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 18.161.111.39, 18.161.111.44, 18.161.111.80, ...\n",
      "Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|18.161.111.39|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1533290 (1.5M) [application/x-gzip]\n",
      "Saving to: ‘./text_files.tar.gz’\n",
      "\n",
      "./text_files.tar.gz 100%[===================>]   1.46M  1.40MB/s    in 1.0s    \n",
      "\n",
      "2023-02-19 04:23:19 (1.40 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
      "\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.6\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
      "file_len = 2579888\n"
     ]
    }
   ],
   "source": [
    "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
    "! tar -xzf text_files.tar.gz\n",
    "! pip install unidecode\n",
    "! pip install torch\n",
    "\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    " \n",
    "import pdb\n",
    " \n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxBeKeNjJ0NQ",
    "outputId": "75c4bbed-ad54-4a76-91aa-84962fdf6767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "unwatched. His legs were securely bound, but his arms were only tied about \n",
      "the wrists, and his hands were in front of him. He could move them both \n",
      "together, though the bonds were cruelly tight. He p\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    " \n",
    "def random_chunk():\n",
    "  start_index = random.randint(0, file_len - chunk_len)\n",
    "  end_index = start_index + chunk_len + 1\n",
    "  return file[start_index:end_index]\n",
    "  \n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SfZ-3mw1sMk-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "On0_WitWJ99e",
    "outputId": "b0053cfa-7eed-4cd7-f381-2b468bf9e272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "  tensor = torch.zeros(len(string)).long()\n",
    "  for c in range(len(string)):\n",
    "      tensor[c] = all_characters.index(string[c])\n",
    "  return tensor\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYJPTLcaYmfI"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 4: Creating a GRU cell \n",
    "\n",
    "---\n",
    "\n",
    "Custom GRU class using the same parameters as the built-in Pytorch class does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aavAv50ZKQ-F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRU(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers):\n",
    "    super(GRU, self).__init__()\n",
    "    # make the weights based on the parameter sizes\n",
    "    \n",
    "    self.num_layers = num_layers\n",
    "    self.HR = nn.ModuleList([nn.Linear(hidden_size, hidden_size) \n",
    "                                       for k in range(num_layers)])\n",
    "    self.IR = nn.ModuleList([nn.Linear(input_size, hidden_size) if k == 0 \n",
    "                             else nn.Linear(hidden_size, hidden_size) \n",
    "                             for k in range(num_layers)])\n",
    "    \n",
    "    self.HZ = nn.ModuleList([nn.Linear(hidden_size, hidden_size) \n",
    "                                       for k in range(num_layers)])\n",
    "    self.IZ = nn.ModuleList([nn.Linear(input_size, hidden_size) if k == 0 \n",
    "                             else nn.Linear(hidden_size, hidden_size) \n",
    "                             for k in range(num_layers)])\n",
    "    \n",
    "    self.HH = nn.ModuleList([nn.Linear(hidden_size, hidden_size) \n",
    "                                       for k in range(num_layers)])\n",
    "    self.IH = nn.ModuleList([nn.Linear(input_size, hidden_size) if k == 0 \n",
    "                             else nn.Linear(hidden_size, hidden_size) \n",
    "                             for k in range(num_layers)])\n",
    "  \n",
    "  def forward(self, inputs, hidden):\n",
    "\n",
    "    updated_hiddens = []\n",
    "    xT = inputs\n",
    "\n",
    "    for k, prev in enumerate(hidden):\n",
    "      r_t = torch.sigmoid(self.IR[k](xT) + self.HR[k](prev))\n",
    "      z_t = torch.sigmoid(self.IZ[k](xT) + self.HR[k](prev))\n",
    "      n_t = torch.tanh(self.IH[k](xT) + r_t * self.HH[k](prev))\n",
    "      h_t = (1 - z_t) * n_t + z_t * prev\n",
    "\n",
    "      xT = h_t\n",
    "      updated_hiddens.append(h_t.unsqueeze(0))\n",
    "    return xT, torch.cat(updated_hiddens, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtXdX-B_WiAY"
   },
   "source": [
    "---\n",
    "\n",
    "##  Part 1: Building a sequence to sequence model\n",
    "\n",
    "---\n",
    "\n",
    "Great! We have the data in a useable form. We can switch out which text file we are reading from and trying to simulate.\n",
    "\n",
    "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d6tNdEnzWj5F"
   },
   "outputs": [],
   "source": [
    "# PART 1\n",
    "\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "    super(RNN, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    \n",
    "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "    self.gru = GRU(hidden_size, hidden_size, self.n_layers)\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  # typically expects a sequence, but this function takes a single character (tensor)\n",
    "  # with the hidden (tensor)\n",
    "  # should return the updated hidden and tensor of probabilities same size as input_char\n",
    "  \n",
    "  def forward(self, input_char, hidden):\n",
    "    output = self.embedding(input_char).unsqueeze(0)\n",
    "    output = F.relu(output)\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "    output = self.out(output)\n",
    "    return output, hidden\n",
    "    \n",
    "  def init_hidden(self):\n",
    "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hrhXghEPKD-5"
   },
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "  chunk = random_chunk()\n",
    "  inp = char_tensor(chunk[:-1])\n",
    "  target = char_tensor(chunk[1:])\n",
    "  return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHTawWBLxJlV"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpiGObbBX0Mr"
   },
   "source": [
    "## Part 2: Sample text and Training information\n",
    "\n",
    "---\n",
    "\n",
    "We now want to be able to train our network, and sample text after training.\n",
    "\n",
    "This function outlines how training a sequence style network goes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2ALC3Pf8Kbsi"
   },
   "outputs": [],
   "source": [
    "# PART 2\n",
    "\n",
    "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n",
    "def train(inp, target):\n",
    "  # initialize hidden layers, set up gradient and loss \n",
    "  decoder_optimizer.zero_grad()\n",
    "  hidden = decoder.init_hidden()\n",
    "  loss = 0\n",
    "  length = len(inp)\n",
    "\n",
    "  # how to handle a sequence of information. \n",
    "  # Use a for loop in the training function to iterate over characters\n",
    "  for character, letter in zip(inp, target):\n",
    "    y_hat, hidden = decoder(character, hidden)\n",
    "    loss += criterion(y_hat, letter.unsqueeze(0))\n",
    "\n",
    "  loss.backward()\n",
    "  decoder_optimizer.step()\n",
    "  return loss.item() / length    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN06NUu3YRlz"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 3: Sample text and Training information\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "B-bp-OZ1KjNh"
   },
   "outputs": [],
   "source": [
    "# PART 3\n",
    "\n",
    "def sample_outputs(output, temperature):\n",
    "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
    "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
    "\n",
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "  # initialize hidden state, initialize other useful variables\n",
    "  # /use a for loop to iterate over the 100 characters    \n",
    "  with torch.no_grad():\n",
    "    hidden_state = decoder.init_hidden()\n",
    "    generated_text = prime_str\n",
    "    prediction = ''\n",
    "\n",
    "    for k in range(predict_len):\n",
    "\n",
    "      if k < len(prime_str):\n",
    "        prime = char_tensor(prime_str[k])\n",
    "        output, hidden_state = decoder(prime.squeeze(), hidden_state)\n",
    "      else: \n",
    "        generated_text += all_characters[prediction]\n",
    "        output, hidden_state = decoder(prediction, hidden_state)\n",
    "\n",
    "      prediction = sample_outputs(output.squeeze(0).squeeze(0), temperature)\n",
    "\n",
    "    generated_text += all_characters[prediction]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFS2bpHSZEU6"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Part 5: Run it and generate some text!\n",
    "\n",
    "---\n",
    "I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-nXFeCmdKodw"
   },
   "outputs": [],
   "source": [
    "# PART 5\n",
    "\n",
    "import time\n",
    "n_epochs = 5000\n",
    "print_every = 200\n",
    "plot_every = 10\n",
    "hidden_size = 200\n",
    "n_layers = 3\n",
    "lr = 0.001\n",
    " \n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xKfozqw-6eqb",
    "outputId": "193ee362-e4c1-46ad-eb15-aa2f44f9b26c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111.99504733085632 (200 4%) 2.5684]\n",
      "Wheres wong anld and gard he hot, hur the wel \n",
      "tod cI hasy geft tand, reiln \n",
      "vas annd Imer or wecolt  \n",
      "\n",
      "[218.788715839386 (400 8%) 2.0271]\n",
      "Whainins, atse sice fon.' Treat even endorlen'sbis onet in the bow the wat pokere undase \n",
      "the herpore \n",
      "\n",
      "[324.88765835762024 (600 12%) 1.9141]\n",
      "Whilled, and nowly, lessed yourres in't loth he deally \n",
      "the romoung. \n",
      "wive \n",
      "dowr oumale his lenk fall \n",
      "\n",
      "[432.4325795173645 (800 16%) 1.8566]\n",
      "Wherd streen a smape grownds houred the were on the like-and did cits were in shickth sparn, unkest s \n",
      "\n",
      "[547.4234738349915 (1000 20%) 1.8525]\n",
      "Why fars, but luded longersh, that could gland. 'I reyestrest and \n",
      "striling. 'It was the more, you he \n",
      "\n",
      "[663.5982022285461 (1200 24%) 1.4807]\n",
      "What way passed though to be \n",
      "were way \n",
      "sword epress sat the had obe have don't go not of the for ene \n",
      "\n",
      "[775.0870950222015 (1400 28%) 1.4664]\n",
      "When when the sumpers was or you do us; creary of some was \n",
      "not \n",
      "hope fire. \n",
      "\n",
      "Frodo \n",
      "\n",
      "arching to from \n",
      "\n",
      "[885.7591338157654 (1600 32%) 1.4934]\n",
      "Where word a trecell. Over. There ridden. \n",
      "\n",
      "I did behind could him. The \n",
      "\n",
      "vale there is the was all t \n",
      "\n",
      "[997.4093997478485 (1800 36%) 1.7345]\n",
      "Where like \n",
      "will speaking resellents that for \n",
      "with they forny, \n",
      "\n",
      "\n",
      "there way enemer. As the preather  \n",
      "\n",
      "[1105.6285259723663 (2000 40%) 1.3799]\n",
      "White \n",
      "mistry of the \n",
      "brice out and they siden his have \n",
      "went where neither fam up and he can the Riv \n",
      "\n",
      "[1212.5768151283264 (2200 44%) 1.5109]\n",
      "Whifely heard, Isilder it. He seen the feen, and the San the shapen they mith the least. \n",
      "\n",
      "'We call \n",
      " \n",
      "\n",
      "[1319.908971786499 (2400 48%) 1.4724]\n",
      "While for he must now when are no horse by a cloud for some many you well. As guest world known will  \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-33d3216d35f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-fc77c90f8c23>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(inp, target)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# n_epochs = 2000\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  input, targ = random_training_set()\n",
    "  loss_ = train(input, targ)       \n",
    "  loss_avg += loss_\n",
    "\n",
    "  if epoch % print_every == 0:\n",
    "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
    "      print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "  if epoch % plot_every == 0:\n",
    "      all_losses.append(loss_avg / plot_every)\n",
    "      loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ee0so6aKJ5L8",
    "outputId": "b49cc274-e1f0-4a59-8d6d-e9eab85a11a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I \n",
      " I day \n",
      "early \n",
      "and they flasing to thin, \n",
      "and that the tought out of the morning the \n",
      "boot of the tongue hornly \n",
      "fage and percally. If I said: 'We male they down and still side,' said Fard, After at th \n",
      "\n",
      " he\n",
      " he last looked it stride: in my land. Then the Baggin begany with the that is the Half, you said the hall not looked is a down \n",
      "you go to they sent about the last went of pons; and not her wells of se \n",
      "\n",
      " he\n",
      " heard away one to a \n",
      "morner and weel \n",
      "bent to be long his not left of the end they way, \n",
      "the Stone Merry of the puttless with a grieved fles ahead, though the emvers in the Moon that the Ding \n",
      "felt, a \n",
      "\n",
      " ca\n",
      " call, I must not we wes and until let my for his, and all the darmed \n",
      "the black the slus the Falken there, the susped for the holly \n",
      "to the rose with \n",
      "such the Fraggons of my let they were \n",
      "sound sick \n",
      "\n",
      " ca\n",
      " came the other of distance his feet, and \n",
      "then them. Mord the rimposs. Which the true the fath's fear what with I should had in we \n",
      "would return and aid. \"They had been on up. I don't we'm even must s \n",
      "\n",
      " ca\n",
      " came the Sun little were a liyed Forly that all, Morruntal should in and it or the \n",
      "now on the others other pot as from the Death and lie now rest was some road. But that they the name as me by they h \n",
      "\n",
      " G\n",
      " Gondor way. \n",
      "\n",
      "There we mountain, unter fain speak green before it water from the turning to grey day Eyes. We \n",
      "may under \n",
      "they can the many pass they ded between in the turned far for \n",
      "Frodo, when doi \n",
      "\n",
      " G\n",
      " Gandalf and we on the widder from it \n",
      "was reaches to a may mean ends. Where need that the Tryas from they clower take luck. Merry. The tall the grey, done, and the night. It we that don't was \n",
      "until t \n",
      "\n",
      " G\n",
      " Gollumileful in the glousel toward that that the said able his eye \n",
      "been of m(n from the \n",
      "company before I finger Merry, quicked in his high. As shout are and grey with did setmen now some discapters  \n",
      "\n",
      " lo\n",
      " long us still seemed dyen sat a watering and depry other for that was there hurril some letter up waters been to \n",
      "the went will weary. Fare a guess far must \n",
      "be to know stone the trying and the dark s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
    "  start = random.randint(0,len(start_strings)-1)\n",
    "  print(start_strings[start])\n",
    "#   all_characters.index(string[c])\n",
    "  print(evaluate(start_strings[start], 200), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJhgDc2IauPE"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 6: Generate output on a different dataset\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjLpcdPKJ-Un"
   },
   "source": [
    "I trained my model on tweets about president Trump during the 2020 election. The model learned proper names and grammatical and syntactic structure well, but not all the sentences make very much sense and a lot of the words look made up. The model may have had trouble training because some tweets where not in English, and some contained non-latin or alphanumerical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAViBrUbBSer",
    "outputId": "c87f3ba1-4e03-4fd7-b629-4ee4b02f2d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Length = 1211307\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "num_char = len(all_characters)\n",
    "file_name = './text_files/tweets.txt'\n",
    "text_file = unidecode.unidecode(open(file_name).read())\n",
    "length = len(text_file)\n",
    "\n",
    "print('File Length =', length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lvXx5qQFBZM9"
   },
   "outputs": [],
   "source": [
    "n_epochs = 2001\n",
    "print_every = 200\n",
    "hidden_size = 200\n",
    "n_layers = 3\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JULjXAXnBB5v",
    "outputId": "13f9ded2-25e3-48fb-8856-e7836a4b4d9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116.40340971946716 (200 9%) 2.4978]\n",
      "Whe, the wo wen anl the an let \n",
      "hehe his ais rgoB soles lod etlius kanotl \n",
      "nis aind clcirguk vithd ce \n",
      "\n",
      "[231.13353061676025 (400 19%) 2.1435]\n",
      "When slado fred thing do shand werreet he the the the \n",
      "the ridwen the like sat mere the shems, do fre \n",
      "\n",
      "[344.92059111595154 (600 29%) 1.8615]\n",
      "Wh mast haf for for and we weras bening.' Thit a darly \n",
      "sunt.wh stert about aly his hieghing ha>ome p \n",
      "\n",
      "[461.94024634361267 (800 39%) 1.8642]\n",
      "Wher they the Frossess and agand for and \n",
      "louds sdaws. So ethert with houve a speme in the buat the s \n",
      "\n",
      "[577.1062970161438 (1000 49%) 1.7767]\n",
      "Whing way had not Men them med. But hought on we \n",
      "reat the cold old crongain light in the fern inroth \n",
      "\n",
      "[692.2199897766113 (1200 59%) 1.6700]\n",
      "Why brownly thing's nespay. Then more there matt, and promisil they shast, and they with brown \n",
      "that  \n",
      "\n",
      "[807.2993059158325 (1400 69%) 1.7416]\n",
      "Where out. \n",
      "\n",
      "'It a sunting. Where decome, as that pery turred of this stay a stearfart,' said they gr \n",
      "\n",
      "[921.6943571567535 (1600 79%) 1.6347]\n",
      "Where times heart for seam to let a chose fair lile the green aid the Morifor a diver, We plast befor \n",
      "\n",
      "[1044.9621090888977 (1800 89%) 1.7256]\n",
      "Wher earer and \n",
      "perseaving and they with on the \n",
      "don, and had lands of the should \n",
      "gone who she was t \n",
      "\n",
      "[1166.7788331508636 (2000 99%) 1.4579]\n",
      "When conter him well all been reeding \n",
      "Pippin \n",
      "with \n",
      "of the mains to a stood; and a like, and some fr \n",
      "\n",
      " Th\n",
      " That strong that the Lord of the Namber. You were here, bounth be \n",
      "shall me down. A still our ridered on the \n",
      "\n",
      "\n",
      "\n",
      "against be ender. And for an him take in the \n",
      "here we grees, and clid and lay right \n",
      "an \n",
      "\n",
      " G\n",
      " Gast of come to stood made, even. The many shall mome \n",
      "Chorted behind by store. He spoke, and \n",
      "the mandard of your warning faces of the Topting were \n",
      "riders, and as the sure of the Went with they came \n",
      "\n",
      " ra\n",
      " rain and feet his is \n",
      "retwemp. But the skall more of the dies roads of him canmed, who with a housed again of the sat great seen \n",
      "away. Of thinklow, and white part of Then the guardly to but best \n",
      "goo \n",
      "\n",
      " ca\n",
      " came i spilloke be horse of then the great we great again the Ring into his handly be which spelt field, and stead. \n",
      "\n",
      "The hope a gate a should be on, me \n",
      "of a showed they kindled and was no dear, of i \n",
      "\n",
      " ra\n",
      " rawing than that he was down \n",
      "and still storftry, nor far what he caming under laived beathing and swellow stood of micken and the grow here regoaning his been \n",
      "them, \n",
      "saoked in the Brain a shiloble o \n",
      "\n",
      " lo\n",
      " looked of some for the thing were haard here way, and his \n",
      "white starce. I Have see at the stargled. \n",
      "\n",
      "The diver the sun come of the knew \n",
      "now, mim, untilling down stay in a came. \n",
      "\n",
      "'Sud some that the \n",
      "\n",
      " ra\n",
      " raimed and not see him, walked or through the Broad, and Sustion in the Forst; and \n",
      "where speal the destants lefts the worn caught we sword of see alrow a middle, but so a made of the him, great of th \n",
      "\n",
      " ca\n",
      " came that he turned that the \n",
      "riftains words see which \n",
      "stepting suddened to the Ersy, went \n",
      "they she saw the wants dark any might shim. \n",
      "\n",
      "'I house of him, he for its be him were speak lont, \n",
      "were gre \n",
      "\n",
      " G\n",
      " Gore mark, and his shore with then Merry \n",
      "on \n",
      "their hat middle, and the \n",
      "aragward to we than reed comsing his resently, but they feet of Sauron on lay \n",
      "wore the some, been. There with a bound he was f \n",
      "\n",
      " wh\n",
      " which like the brighth, and fimentle. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Who with thing on the Companather. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The slived the light our \n",
      "came becauch, but a might, and \n",
      "sword think wither, and him Gimney ride still f \n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder = RNN(num_char, hidden_size, num_char, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "start = time.time()\n",
    "total_loss = []\n",
    "\n",
    "for e in range(1, n_epochs + 1):\n",
    "  input, targ = random_training_set()\n",
    "  loss_ = train(input, targ)        \n",
    "\n",
    "  if e % print_every == 0:\n",
    "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, e, e / n_epochs * 100, loss_))\n",
    "      print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "for i in range(10):\n",
    "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
    "  start = random.randint(0,len(start_strings)-1)\n",
    "  print(start_strings[start])\n",
    "#   all_characters.index(string[c])\n",
    "  print(evaluate(start_strings[start], 200), '\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
