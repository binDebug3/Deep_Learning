{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGPzaxTp75Ls"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "### Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "### Deliverable:\n",
        "- Fill in the code for the RNN (using PyTorch's built-in GRU).\n",
        "- Fill in the training loop\n",
        "- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n",
        "- Implement your own GRU cell.\n",
        "- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n",
        "\n",
        "### Grading Standards:\n",
        "- 20% Implementation the RNN\n",
        "- 20% Implementation training loop\n",
        "- 20% Implementation of evaluation loop\n",
        "- 20% Implementation of your own GRU cell\n",
        "- 20% Training of your RNN on a domain of your choice\n",
        "\n",
        "### Tips:\n",
        "- Read through all the helper functions, run them, and make sure you understand what they are doing\n",
        "- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n",
        "- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n",
        "\n",
        "### Example Output:\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling \n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (Take note that you will not be implementing the encoder part of this tutorial.)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bdZWxvJrsx",
        "outputId": "2ab0060d-d673-4750-95bc-e0183ddf06b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-19 04:23:17--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 18.215.222.38, 52.205.194.150, 52.206.193.161, ...\n",
            "Connecting to piazza.com (piazza.com)|18.215.222.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2023-02-19 04:23:18--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 18.161.111.39, 18.161.111.44, 18.161.111.80, ...\n",
            "Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|18.161.111.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  1.40MB/s    in 1.0s    \n",
            "\n",
            "2023-02-19 04:23:19 (1.40 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
            "file_len = 2579888\n"
          ]
        }
      ],
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxBeKeNjJ0NQ",
        "outputId": "75c4bbed-ad54-4a76-91aa-84962fdf6767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "unwatched. His legs were securely bound, but his arms were only tied about \n",
            "the wrists, and his hands were in front of him. He could move them both \n",
            "together, though the bonds were cruelly tight. He p\n"
          ]
        }
      ],
      "source": [
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "SfZ-3mw1sMk-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On0_WitWJ99e",
        "outputId": "b0053cfa-7eed-4cd7-f381-2b468bf9e272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ]
        }
      ],
      "source": [
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell \n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Create a custom GRU cell\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aavAv50ZKQ-F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    # make the weights based on the parameter sizes\n",
        "    \n",
        "    self.num_layers = num_layers\n",
        "    self.HR = nn.ModuleList([nn.Linear(hidden_size, hidden_size) \n",
        "                                       for k in range(num_layers)])\n",
        "    self.IR = nn.ModuleList([nn.Linear(input_size, hidden_size) if k == 0 \n",
        "                             else nn.Linear(hidden_size, hidden_size) \n",
        "                             for k in range(num_layers)])\n",
        "    \n",
        "    self.HZ = nn.ModuleList([nn.Linear(hidden_size, hidden_size) \n",
        "                                       for k in range(num_layers)])\n",
        "    self.IZ = nn.ModuleList([nn.Linear(input_size, hidden_size) if k == 0 \n",
        "                             else nn.Linear(hidden_size, hidden_size) \n",
        "                             for k in range(num_layers)])\n",
        "    \n",
        "    self.HH = nn.ModuleList([nn.Linear(hidden_size, hidden_size) \n",
        "                                       for k in range(num_layers)])\n",
        "    self.IH = nn.ModuleList([nn.Linear(input_size, hidden_size) if k == 0 \n",
        "                             else nn.Linear(hidden_size, hidden_size) \n",
        "                             for k in range(num_layers)])\n",
        "  \n",
        "  def forward(self, inputs, hidden):\n",
        "    # Each layer does the following:\n",
        "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "\n",
        "    updated_hiddens = []\n",
        "    xT = inputs\n",
        "\n",
        "    for k, prev in enumerate(hidden):\n",
        "      r_t = torch.sigmoid(self.IR[k](xT) + self.HR[k](prev))\n",
        "      z_t = torch.sigmoid(self.IZ[k](xT) + self.HR[k](prev))\n",
        "      n_t = torch.tanh(self.IH[k](xT) + r_t * self.HH[k](prev))\n",
        "      h_t = (1 - z_t) * n_t + z_t * prev\n",
        "\n",
        "      xT = h_t\n",
        "      updated_hiddens.append(h_t.unsqueeze(0))\n",
        "    return xT, torch.cat(updated_hiddens, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d6tNdEnzWj5F"
      },
      "outputs": [],
      "source": [
        "# PART 1\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = GRU(hidden_size, hidden_size, self.n_layers)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  # typically expects a sequence, but this function takes a single character (tensor)\n",
        "  # with the hidden (tensor)\n",
        "  # should return the updated hidden and tensor of probabilities same size as input_char\n",
        "  \n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that \n",
        "    # properly uses the output of the GRU\n",
        "    output = self.embedding(input_char).unsqueeze(0)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.out(output)\n",
        "    return output, hidden\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hrhXghEPKD-5"
      },
      "outputs": [],
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "wHTawWBLxJlV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes. \n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2ALC3Pf8Kbsi"
      },
      "outputs": [],
      "source": [
        "# PART 2\n",
        "\n",
        "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n",
        "def train(inp, target):\n",
        "  # initialize hidden layers, set up gradient and loss \n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden = decoder.init_hidden()\n",
        "  loss = 0\n",
        "  length = len(inp)\n",
        "\n",
        "  # how to handle a sequence of information. \n",
        "  # Use a for loop in the training function to iterate over characters\n",
        "  for character, letter in zip(inp, target):\n",
        "    y_hat, hidden = decoder(character, hidden)\n",
        "    loss += criterion(y_hat, letter.unsqueeze(0))\n",
        "\n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "  return loss.item() / length    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B-bp-OZ1KjNh"
      },
      "outputs": [],
      "source": [
        "# PART 3\n",
        "\n",
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  # initialize hidden state, initialize other useful variables\n",
        "  # /use a for loop to iterate over the 100 characters    \n",
        "  with torch.no_grad():\n",
        "    hidden_state = decoder.init_hidden()\n",
        "    generated_text = prime_str\n",
        "    prediction = ''\n",
        "\n",
        "    for k in range(predict_len):\n",
        "\n",
        "      if k < len(prime_str):\n",
        "        prime = char_tensor(prime_str[k])\n",
        "        output, hidden_state = decoder(prime.squeeze(), hidden_state)\n",
        "      else: \n",
        "        generated_text += all_characters[prediction]\n",
        "        output, hidden_state = decoder(prediction, hidden_state)\n",
        "\n",
        "      prediction = sample_outputs(output.squeeze(0).squeeze(0), temperature)\n",
        "\n",
        "    generated_text += all_characters[prediction]\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**TODO:** \n",
        "\n",
        "**DONE:**\n",
        "* Create some cool output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string:\n",
        "\n",
        "---\n",
        "\n",
        " G:\n",
        " \n",
        " Gandalf was decrond. \n",
        "'All have lord you. Forward the road at least walk this is stuff, and \n",
        "went to the long grey housel-winding and kindled side was a sleep pleasuring, I do long \n",
        "row hrough. In  \n",
        "\n",
        " lo:\n",
        " \n",
        " lost death it. \n",
        "'The last of the gatherings and take you,' said Aragorn, shining out of the Gate. \n",
        "'Yes, as you there were remembaused to seen their pass, when? What \n",
        "said here, such seven an the sear \n",
        "\n",
        " lo:\n",
        " \n",
        " low, and frod to keepn \n",
        "Came of their most. But here priced doubtless to an Sam up is \n",
        "masters; he left hor as they are looked. And he could now the long to stout in the right fro horseless of \n",
        "the like \n",
        "\n",
        " I:\n",
        " \n",
        " I had been the \n",
        "in his eyes with the perushed to lest, if then only the ring and the legended \n",
        "of the less of the long they which as the \n",
        "enders of Orcovered and smood, and the p \n",
        "\n",
        " I:\n",
        " \n",
        " I they were not the lord of the hoomes. \n",
        "Home already well from the Elves. And he sat strength, and we \n",
        "housed out of the good of the days to the mountains from his perith. \n",
        "\n",
        "'Yess! Where though as if  \n",
        "\n",
        " Th:\n",
        " \n",
        " There yarden \n",
        "you would guard the hoor might. Far and then may was \n",
        "croties, too began to see the drumbred many line \n",
        "and was then hoard walk and they heart, and the chair of the \n",
        "Ents of way, might was \n",
        "\n",
        " G:\n",
        " \n",
        " Gandalf \n",
        "been lat of less the round of the stump; both and seemed to the trees and perished they \n",
        "lay are speered the less; and the wind the steep and have to she \n",
        "precious. There was in the oonly went \n",
        "\n",
        " wh:\n",
        " \n",
        " which went out of the door. \n",
        "Hull the King and of the The days of his brodo \n",
        "stumbler of the windard was a thing there, then it been shining langing \n",
        "to him poor land. They hands; though they seemed ou \n",
        "\n",
        " ra:\n",
        " \n",
        " rather,' have all the least deather \n",
        "down of the truven beginning to the house of sunk. \n",
        "'Nark shorts of the Eyes of the Gate your great nothing as Eret. \n",
        "'I wander trust horn, and there were not, it  \n",
        "\n",
        " I:\n",
        " \n",
        " I can have no mind \n",
        "together! Where don't may had one may little blung \n",
        "terrible to tales. And turn and Gandalf shall be not to as only the Cattring \n",
        "not stopped great the out them forms. On they she lo \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-nXFeCmdKodw"
      },
      "outputs": [],
      "source": [
        "# PART 5\n",
        "\n",
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xKfozqw-6eqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "193ee362-e4c1-46ad-eb15-aa2f44f9b26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[111.99504733085632 (200 4%) 2.5684]\n",
            "Wheres wong anld and gard he hot, hur the wel \n",
            "tod cI hasy geft tand, reiln \n",
            "vas annd Imer or wecolt  \n",
            "\n",
            "[218.788715839386 (400 8%) 2.0271]\n",
            "Whainins, atse sice fon.' Treat even endorlen'sbis onet in the bow the wat pokere undase \n",
            "the herpore \n",
            "\n",
            "[324.88765835762024 (600 12%) 1.9141]\n",
            "Whilled, and nowly, lessed yourres in't loth he deally \n",
            "the romoung. \n",
            "wive \n",
            "dowr oumale his lenk fall \n",
            "\n",
            "[432.4325795173645 (800 16%) 1.8566]\n",
            "Wherd streen a smape grownds houred the were on the like-and did cits were in shickth sparn, unkest s \n",
            "\n",
            "[547.4234738349915 (1000 20%) 1.8525]\n",
            "Why fars, but luded longersh, that could gland. 'I reyestrest and \n",
            "striling. 'It was the more, you he \n",
            "\n",
            "[663.5982022285461 (1200 24%) 1.4807]\n",
            "What way passed though to be \n",
            "were way \n",
            "sword epress sat the had obe have don't go not of the for ene \n",
            "\n",
            "[775.0870950222015 (1400 28%) 1.4664]\n",
            "When when the sumpers was or you do us; creary of some was \n",
            "not \n",
            "hope fire. \n",
            "\n",
            "Frodo \n",
            "\n",
            "arching to from \n",
            "\n",
            "[885.7591338157654 (1600 32%) 1.4934]\n",
            "Where word a trecell. Over. There ridden. \n",
            "\n",
            "I did behind could him. The \n",
            "\n",
            "vale there is the was all t \n",
            "\n",
            "[997.4093997478485 (1800 36%) 1.7345]\n",
            "Where like \n",
            "will speaking resellents that for \n",
            "with they forny, \n",
            "\n",
            "\n",
            "there way enemer. As the preather  \n",
            "\n",
            "[1105.6285259723663 (2000 40%) 1.3799]\n",
            "White \n",
            "mistry of the \n",
            "brice out and they siden his have \n",
            "went where neither fam up and he can the Riv \n",
            "\n",
            "[1212.5768151283264 (2200 44%) 1.5109]\n",
            "Whifely heard, Isilder it. He seen the feen, and the San the shapen they mith the least. \n",
            "\n",
            "'We call \n",
            " \n",
            "\n",
            "[1319.908971786499 (2400 48%) 1.4724]\n",
            "While for he must now when are no horse by a cloud for some many you well. As guest world known will  \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-33d3216d35f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-fc77c90f8c23>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(inp, target)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  input, targ = random_training_set()\n",
        "  loss_ = train(input, targ)       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ee0so6aKJ5L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49cc274-e1f0-4a59-8d6d-e9eab85a11a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I \n",
            " I day \n",
            "early \n",
            "and they flasing to thin, \n",
            "and that the tought out of the morning the \n",
            "boot of the tongue hornly \n",
            "fage and percally. If I said: 'We male they down and still side,' said Fard, After at th \n",
            "\n",
            " he\n",
            " he last looked it stride: in my land. Then the Baggin begany with the that is the Half, you said the hall not looked is a down \n",
            "you go to they sent about the last went of pons; and not her wells of se \n",
            "\n",
            " he\n",
            " heard away one to a \n",
            "morner and weel \n",
            "bent to be long his not left of the end they way, \n",
            "the Stone Merry of the puttless with a grieved fles ahead, though the emvers in the Moon that the Ding \n",
            "felt, a \n",
            "\n",
            " ca\n",
            " call, I must not we wes and until let my for his, and all the darmed \n",
            "the black the slus the Falken there, the susped for the holly \n",
            "to the rose with \n",
            "such the Fraggons of my let they were \n",
            "sound sick \n",
            "\n",
            " ca\n",
            " came the other of distance his feet, and \n",
            "then them. Mord the rimposs. Which the true the fath's fear what with I should had in we \n",
            "would return and aid. \"They had been on up. I don't we'm even must s \n",
            "\n",
            " ca\n",
            " came the Sun little were a liyed Forly that all, Morruntal should in and it or the \n",
            "now on the others other pot as from the Death and lie now rest was some road. But that they the name as me by they h \n",
            "\n",
            " G\n",
            " Gondor way. \n",
            "\n",
            "There we mountain, unter fain speak green before it water from the turning to grey day Eyes. We \n",
            "may under \n",
            "they can the many pass they ded between in the turned far for \n",
            "Frodo, when doi \n",
            "\n",
            " G\n",
            " Gandalf and we on the widder from it \n",
            "was reaches to a may mean ends. Where need that the Tryas from they clower take luck. Merry. The tall the grey, done, and the night. It we that don't was \n",
            "until t \n",
            "\n",
            " G\n",
            " Gollumileful in the glousel toward that that the said able his eye \n",
            "been of m(n from the \n",
            "company before I finger Merry, quicked in his high. As shout are and grey with did setmen now some discapters  \n",
            "\n",
            " lo\n",
            " long us still seemed dyen sat a watering and depry other for that was there hurril some letter up waters been to \n",
            "the went will weary. Fare a guess far must \n",
            "be to know stone the trying and the dark s \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I trained my model on tweets about president Trump during the 2020 election. The model learned proper names and grammatical and syntactic structure well, but not all the sentences make very much sense and a lot of the words look made up. The model may have had trouble training because some tweets where not in English, and some contained non-latin or alphanumerical characters."
      ],
      "metadata": {
        "id": "kjLpcdPKJ-Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_characters = string.printable\n",
        "num_char = len(all_characters)\n",
        "file_name = './text_files/tweets.txt'\n",
        "text_file = unidecode.unidecode(open(file_name).read())\n",
        "length = len(text_file)\n",
        "\n",
        "print('File Length =', length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAViBrUbBSer",
        "outputId": "c87f3ba1-4e03-4fd7-b629-4ee4b02f2d3d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Length = 1211307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 2001\n",
        "print_every = 200\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "lvXx5qQFBZM9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = RNN(num_char, hidden_size, num_char, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "total_loss = []\n",
        "\n",
        "for e in range(1, n_epochs + 1):\n",
        "  input, targ = random_training_set()\n",
        "  loss_ = train(input, targ)        \n",
        "\n",
        "  if e % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, e, e / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JULjXAXnBB5v",
        "outputId": "13f9ded2-25e3-48fb-8856-e7836a4b4d9e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[116.40340971946716 (200 9%) 2.4978]\n",
            "Whe, the wo wen anl the an let \n",
            "hehe his ais rgoB soles lod etlius kanotl \n",
            "nis aind clcirguk vithd ce \n",
            "\n",
            "[231.13353061676025 (400 19%) 2.1435]\n",
            "When slado fred thing do shand werreet he the the the \n",
            "the ridwen the like sat mere the shems, do fre \n",
            "\n",
            "[344.92059111595154 (600 29%) 1.8615]\n",
            "Wh mast haf for for and we weras bening.' Thit a darly \n",
            "sunt.wh stert about aly his hieghing ha>ome p \n",
            "\n",
            "[461.94024634361267 (800 39%) 1.8642]\n",
            "Wher they the Frossess and agand for and \n",
            "louds sdaws. So ethert with houve a speme in the buat the s \n",
            "\n",
            "[577.1062970161438 (1000 49%) 1.7767]\n",
            "Whing way had not Men them med. But hought on we \n",
            "reat the cold old crongain light in the fern inroth \n",
            "\n",
            "[692.2199897766113 (1200 59%) 1.6700]\n",
            "Why brownly thing's nespay. Then more there matt, and promisil they shast, and they with brown \n",
            "that  \n",
            "\n",
            "[807.2993059158325 (1400 69%) 1.7416]\n",
            "Where out. \n",
            "\n",
            "'It a sunting. Where decome, as that pery turred of this stay a stearfart,' said they gr \n",
            "\n",
            "[921.6943571567535 (1600 79%) 1.6347]\n",
            "Where times heart for seam to let a chose fair lile the green aid the Morifor a diver, We plast befor \n",
            "\n",
            "[1044.9621090888977 (1800 89%) 1.7256]\n",
            "Wher earer and \n",
            "perseaving and they with on the \n",
            "don, and had lands of the should \n",
            "gone who she was t \n",
            "\n",
            "[1166.7788331508636 (2000 99%) 1.4579]\n",
            "When conter him well all been reeding \n",
            "Pippin \n",
            "with \n",
            "of the mains to a stood; and a like, and some fr \n",
            "\n",
            " Th\n",
            " That strong that the Lord of the Namber. You were here, bounth be \n",
            "shall me down. A still our ridered on the \n",
            "\n",
            "\n",
            "\n",
            "against be ender. And for an him take in the \n",
            "here we grees, and clid and lay right \n",
            "an \n",
            "\n",
            " G\n",
            " Gast of come to stood made, even. The many shall mome \n",
            "Chorted behind by store. He spoke, and \n",
            "the mandard of your warning faces of the Topting were \n",
            "riders, and as the sure of the Went with they came \n",
            "\n",
            " ra\n",
            " rain and feet his is \n",
            "retwemp. But the skall more of the dies roads of him canmed, who with a housed again of the sat great seen \n",
            "away. Of thinklow, and white part of Then the guardly to but best \n",
            "goo \n",
            "\n",
            " ca\n",
            " came i spilloke be horse of then the great we great again the Ring into his handly be which spelt field, and stead. \n",
            "\n",
            "The hope a gate a should be on, me \n",
            "of a showed they kindled and was no dear, of i \n",
            "\n",
            " ra\n",
            " rawing than that he was down \n",
            "and still storftry, nor far what he caming under laived beathing and swellow stood of micken and the grow here regoaning his been \n",
            "them, \n",
            "saoked in the Brain a shiloble o \n",
            "\n",
            " lo\n",
            " looked of some for the thing were haard here way, and his \n",
            "white starce. I Have see at the stargled. \n",
            "\n",
            "The diver the sun come of the knew \n",
            "now, mim, untilling down stay in a came. \n",
            "\n",
            "'Sud some that the \n",
            "\n",
            " ra\n",
            " raimed and not see him, walked or through the Broad, and Sustion in the Forst; and \n",
            "where speal the destants lefts the worn caught we sword of see alrow a middle, but so a made of the him, great of th \n",
            "\n",
            " ca\n",
            " came that he turned that the \n",
            "riftains words see which \n",
            "stepting suddened to the Ersy, went \n",
            "they she saw the wants dark any might shim. \n",
            "\n",
            "'I house of him, he for its be him were speak lont, \n",
            "were gre \n",
            "\n",
            " G\n",
            " Gore mark, and his shore with then Merry \n",
            "on \n",
            "their hat middle, and the \n",
            "aragward to we than reed comsing his resently, but they feet of Sauron on lay \n",
            "wore the some, been. There with a bound he was f \n",
            "\n",
            " wh\n",
            " which like the brighth, and fimentle. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Who with thing on the Companather. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The slived the light our \n",
            "came becauch, but a might, and \n",
            "sword think wither, and him Gimney ride still f \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}